# -*- coding: utf-8 -*-
"""soilUNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJyVWeVWT0Y71UBbn8zm5iEIqjGJnJBv
"""

!pip install -q roboflow torch torchvision albumentations tqdm seaborn

from roboflow import Roboflow

rf = Roboflow(api_key="pCdHjiewdPWUH8idtZp7")
project = rf.workspace("blue-halo").project("vegetation-segmentation")
version = project.version(4)
dataset = version.download("yolov8")

from pathlib import Path
DATA_DIR = Path("/content/vegetation-segmentation-4")

from PIL import Image, ImageDraw, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

def yolo_to_mask(img_path, txt_path):
    img = Image.open(img_path).convert("RGB")
    w, h = img.size
    mask = Image.new("L", (w, h), 0)
    draw = ImageDraw.Draw(mask)

    if not txt_path.exists():
        return mask

    for line in txt_path.read_text().splitlines():
        parts = line.split()
        coords = list(map(float, parts[1:]))
        pts = [(coords[i]*w, coords[i+1]*h) for i in range(0, len(coords), 2)]
        if len(pts) >= 3:
            draw.polygon(pts, fill=255)

    return mask

for split in ["train","valid","test"]:
    img_dir = DATA_DIR/split/"images"
    lbl_dir = DATA_DIR/split/"labels"
    mask_dir = DATA_DIR/split/"masks"
    mask_dir.mkdir(exist_ok=True)

    for img in img_dir.glob("*"):
        txt = lbl_dir/(img.stem+".txt")
        mask = yolo_to_mask(img, txt)
        mask.save(mask_dir/(img.stem+".png"))

import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
import numpy as np
from PIL import Image

class VegDataset(Dataset):
    def __init__(self, split):
        self.imgs = list((DATA_DIR/split/"images").glob("*"))
        self.masks = DATA_DIR/split/"masks"

        self.tf = A.Compose([
            A.Resize(256,256),
            A.HorizontalFlip(p=0.5),
            A.Normalize()
        ])

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, i):
        img = np.array(Image.open(self.imgs[i]).convert("RGB"))
        mask = np.array(Image.open(self.masks/(self.imgs[i].stem+".png")))

        aug = self.tf(image=img, mask=mask)
        img = aug["image"].transpose(2,0,1)
        mask = (aug["mask"] > 0).astype("float32")[None]

        return torch.tensor(img), torch.tensor(mask)

import torch.nn as nn
import torch.nn.functional as F

class DoubleConv(nn.Module):
    def __init__(self, i, o):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(i,o,3,1,1),
            nn.BatchNorm2d(o),
            nn.ReLU(),
            nn.Conv2d(o,o,3,1,1),
            nn.BatchNorm2d(o),
            nn.ReLU()
        )
    def forward(self,x): return self.net(x)

class UNetLite(nn.Module):
    def __init__(self):
        super().__init__()
        self.d1 = DoubleConv(3,32)
        self.d2 = DoubleConv(32,64)
        self.d3 = DoubleConv(64,128)
        self.pool = nn.MaxPool2d(2)
        self.b = DoubleConv(128,256)
        self.u3 = DoubleConv(256+128,128)
        self.u2 = DoubleConv(128+64,64)
        self.u1 = DoubleConv(64+32,32)
        self.out = nn.Conv2d(32,1,1)

    def forward(self,x):
        d1 = self.d1(x)
        d2 = self.d2(self.pool(d1))
        d3 = self.d3(self.pool(d2))
        b = self.b(self.pool(d3))
        x = F.interpolate(b,scale_factor=2); x=self.u3(torch.cat([x,d3],1))
        x = F.interpolate(x,scale_factor=2); x=self.u2(torch.cat([x,d2],1))
        x = F.interpolate(x,scale_factor=2); x=self.u1(torch.cat([x,d1],1))
        return torch.sigmoid(self.out(x))

device = "cuda" if torch.cuda.is_available() else "cpu"

train_ds = VegDataset("train")
val_ds   = VegDataset("valid")

train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)
val_dl   = DataLoader(val_ds, batch_size=8)

model = UNetLite().to(device)
opt = torch.optim.AdamW(model.parameters(), lr=1e-3)

def dice_loss(p,t):
    return 1-(2*(p*t).sum()+1e-6)/(p.sum()+t.sum()+1e-6)

loss_fn = lambda p,t: nn.BCELoss()(p,t)+dice_loss(p,t)

from tqdm import tqdm

EPOCHS = 15
for e in range(EPOCHS):
    model.train()
    total = 0
    for x,y in tqdm(train_dl):
        x,y = x.to(device), y.to(device)
        p = model(x)
        loss = loss_fn(p,y)
        opt.zero_grad(); loss.backward(); opt.step()
        total += loss.item()
    print(f"Epoch {e+1}/{EPOCHS} Loss: {total/len(train_dl):.4f}")

# ============================================
# MANUAL TESTING — UPLOAD IMAGE & VISUALIZE
# ============================================

import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
from google.colab import files

# Ensure model is in eval mode
model.eval()

device = "cuda" if torch.cuda.is_available() else "cpu"

# Upload image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

print("Uploaded image:", img_path)

# Image transform (must match training size)
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])

# Load image
img = Image.open(img_path).convert("RGB")
img_tensor = transform(img).unsqueeze(0).to(device)

# Predict
with torch.no_grad():
    pred = model(img_tensor)
    pred_mask = (pred > 0.5).float().squeeze().cpu().numpy()

# Area calculation
total_pixels = pred_mask.size
veg_pixels = np.sum(pred_mask == 1)
non_veg_pixels = total_pixels - veg_pixels

veg_percent = (veg_pixels / total_pixels) * 100
non_veg_percent = 100 - veg_percent

print(f"\nVegetation Area: {veg_percent:.2f}%")
print(f"Non‑Vegetation Area: {non_veg_percent:.2f}%")

# Create overlay
mask_rgb = np.zeros((256, 256, 3), dtype=np.uint8)
mask_rgb[pred_mask == 1] = [0, 255, 0]  # green vegetation

overlay = Image.blend(
    img.resize((256,256)),
    Image.fromarray(mask_rgb),
    alpha=0.45
)

# Visualization
plt.figure(figsize=(15,5))

plt.subplot(1,3,1)
plt.imshow(img)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1,3,2)
plt.imshow(pred_mask, cmap="gray")
plt.title("Predicted Mask")
plt.axis("off")

plt.subplot(1,3,3)
plt.imshow(overlay)
plt.title(f"Vegetation: {veg_percent:.2f}%")
plt.axis("off")

plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate(loader):
    TP=FP=FN=TN=0
    model.eval()
    with torch.no_grad():
        for x,y in loader:
            x,y = x.to(device), y.to(device)
            p = (model(x)>0.5).float()
            TP += (p*y).sum().item()
            FP += (p*(1-y)).sum().item()
            FN += ((1-p)*y).sum().item()
            TN += ((1-p)*(1-y)).sum().item()

    acc = (TP+TN)/(TP+TN+FP+FN)
    iou = TP/(TP+FP+FN+1e-6)
    dice = (2*TP)/(2*TP+FP+FN+1e-6)
    cm = np.array([[TP,FP],[FN,TN]])
    return acc,iou,dice,cm

acc,iou,dice,cm = evaluate(val_dl)
print("Accuracy:",acc,"IoU:",iou,"Dice:",dice)

sns.heatmap(cm,annot=True,fmt=".0f",
            xticklabels=["Veg","Non‑Veg"],
            yticklabels=["Veg","Non‑Veg"])
plt.show()

torch.save(model.state_dict(),"veg_unet_lite.pth")
from google.colab import files
files.download("veg_unet_lite.pth")